{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "oC0ZDI5x29RZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Generate Synthetic Data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000  # Number of normal samples\n",
        "n_features = 10   # Number of features\n",
        "n_anomalies = 50  # Number of anomalies\n",
        "\n",
        "# Normal data (good data)\n",
        "normal_data = np.random.normal(0, 1, (n_samples, n_features))\n",
        "\n",
        "# Anomalies (outliers)\n",
        "anomalies = np.random.uniform(-10, 10, (n_anomalies, n_features))\n",
        "\n",
        "# Combine data\n",
        "data = np.vstack([normal_data, anomalies])\n",
        "labels = np.array([0] * n_samples + [1] * n_anomalies)  # 0 = normal, 1 = anomaly\n",
        "\n",
        "# Step 2: Split Data into Training and Validation Sets\n",
        "# Training set contains only normal data\n",
        "X_train, X_val_normal = train_test_split(normal_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Validation set contains both normal and anomalous data\n",
        "X_val = np.vstack([X_val_normal, anomalies])\n",
        "y_val = np.array([0] * len(X_val_normal) + [1] * len(anomalies))\n",
        "\n",
        "# Step 3: Normalize the Data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "5m2FxCp83IL9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define the Autoencoder Model\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 8\n",
        "autoencoder = Autoencoder(input_dim, hidden_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "OAu_KS6U3UH8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train the Autoencoder\n",
        "num_epochs = 50\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    autoencoder.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = autoencoder(batch)\n",
        "        loss = criterion(output, batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e50D8ato3bVv",
        "outputId": "5f2dcc1d-e11e-4ba6-d17d-24b5f242c89c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 1.0309\n",
            "Epoch [2/50], Loss: 1.0155\n",
            "Epoch [3/50], Loss: 1.0023\n",
            "Epoch [4/50], Loss: 0.9876\n",
            "Epoch [5/50], Loss: 0.9715\n",
            "Epoch [6/50], Loss: 0.9543\n",
            "Epoch [7/50], Loss: 0.9347\n",
            "Epoch [8/50], Loss: 0.9137\n",
            "Epoch [9/50], Loss: 0.8922\n",
            "Epoch [10/50], Loss: 0.8716\n",
            "Epoch [11/50], Loss: 0.8517\n",
            "Epoch [12/50], Loss: 0.8316\n",
            "Epoch [13/50], Loss: 0.8126\n",
            "Epoch [14/50], Loss: 0.7971\n",
            "Epoch [15/50], Loss: 0.7846\n",
            "Epoch [16/50], Loss: 0.7737\n",
            "Epoch [17/50], Loss: 0.7633\n",
            "Epoch [18/50], Loss: 0.7538\n",
            "Epoch [19/50], Loss: 0.7460\n",
            "Epoch [20/50], Loss: 0.7388\n",
            "Epoch [21/50], Loss: 0.7331\n",
            "Epoch [22/50], Loss: 0.7280\n",
            "Epoch [23/50], Loss: 0.7242\n",
            "Epoch [24/50], Loss: 0.7210\n",
            "Epoch [25/50], Loss: 0.7189\n",
            "Epoch [26/50], Loss: 0.7160\n",
            "Epoch [27/50], Loss: 0.7141\n",
            "Epoch [28/50], Loss: 0.7127\n",
            "Epoch [29/50], Loss: 0.7116\n",
            "Epoch [30/50], Loss: 0.7097\n",
            "Epoch [31/50], Loss: 0.7088\n",
            "Epoch [32/50], Loss: 0.7075\n",
            "Epoch [33/50], Loss: 0.7067\n",
            "Epoch [34/50], Loss: 0.7056\n",
            "Epoch [35/50], Loss: 0.7049\n",
            "Epoch [36/50], Loss: 0.7039\n",
            "Epoch [37/50], Loss: 0.7031\n",
            "Epoch [38/50], Loss: 0.7025\n",
            "Epoch [39/50], Loss: 0.7013\n",
            "Epoch [40/50], Loss: 0.7006\n",
            "Epoch [41/50], Loss: 0.6998\n",
            "Epoch [42/50], Loss: 0.6989\n",
            "Epoch [43/50], Loss: 0.6973\n",
            "Epoch [44/50], Loss: 0.6944\n",
            "Epoch [45/50], Loss: 0.6899\n",
            "Epoch [46/50], Loss: 0.6819\n",
            "Epoch [47/50], Loss: 0.6706\n",
            "Epoch [48/50], Loss: 0.6562\n",
            "Epoch [49/50], Loss: 0.6423\n",
            "Epoch [50/50], Loss: 0.6311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Detect Anomalies on the Validation Set\n",
        "autoencoder.eval()\n",
        "with torch.no_grad():\n",
        "    val_output = autoencoder(X_val)\n",
        "    reconstruction_error = torch.mean((X_val - val_output) ** 2, dim=1).numpy()\n",
        "\n",
        "# Set a threshold for anomaly detection (e.g., 95th percentile of reconstruction error)\n",
        "threshold = np.percentile(reconstruction_error, 95)\n",
        "\n",
        "# Detect anomalies\n",
        "anomalies_detected = reconstruction_error > threshold\n",
        "\n",
        "# Evaluate performance\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_val, anomalies_detected, average='binary')\n",
        "print(f'Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GP7CUpr3eYb",
        "outputId": "4f7ab40c-d893-4868-e1e2-40ea42bf186f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.00, Recall: 0.26, F1-Score: 0.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Visualize Results\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(range(len(y_val)), reconstruction_error, c=y_val, cmap='coolwarm', label='True Labels')\n",
        "plt.axhline(threshold, color='black', linestyle='--', label='Threshold')\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Reconstruction Error')\n",
        "plt.title('Anomaly Detection')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fh1wuyHf3myy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIRVbhB63quj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}